[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5001
[33mPress CTRL+C to quit[0m
127.0.0.1 - - [07/Jul/2025 19:35:40] "POST /api/scan_project HTTP/1.1" 200 -
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5001
[33mPress CTRL+C to quit[0m
127.0.0.1 - - [07/Jul/2025 19:53:43] "POST /api/build_armory_index HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 19:54:18] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 19:54:18] "POST /api/build_armory_index HTTP/1.1" 200 -
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
127.0.0.1 - - [07/Jul/2025 19:54:29] "POST /api/scan_project HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 19:54:32] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 19:54:37] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 19:54:43] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 19:54:48] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 19:54:51] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 19:54:59] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 19:55:32] "POST /api/scan_project HTTP/1.1" 200 -
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5001
[33mPress CTRL+C to quit[0m
127.0.0.1 - - [07/Jul/2025 20:09:55] "POST /api/build_armory_index HTTP/1.1" 200 -
Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/server.py", line 52, in <module>
    app.run(debug=False, port=5001)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/app.py", line 620, in run
    cli.show_server_banner(self.debug, self.name)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/cli.py", line 756, in show_server_banner
    click.echo(f" * Serving Flask app '{app_import_path}'")
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/click/utils.py", line 322, in echo
    file.flush()
BrokenPipeError: [Errno 32] Broken pipe
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
BrokenPipeError: [Errno 32] Broken pipe
Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/server.py", line 52, in <module>
    app.run(debug=False, port=5001)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/app.py", line 620, in run
    cli.show_server_banner(self.debug, self.name)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/cli.py", line 756, in show_server_banner
    click.echo(f" * Serving Flask app '{app_import_path}'")
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/click/utils.py", line 322, in echo
    file.flush()
BrokenPipeError: [Errno 32] Broken pipe
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
BrokenPipeError: [Errno 32] Broken pipe
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5001
[33mPress CTRL+C to quit[0m
127.0.0.1 - - [07/Jul/2025 20:18:03] "POST /api/build_armory_index HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:19:29] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:19:29] "POST /api/build_armory_index HTTP/1.1" 200 -
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
127.0.0.1 - - [07/Jul/2025 20:19:43] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:19:45] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:19:47] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:19:59] "POST /api/scan_project HTTP/1.1" 200 -
Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/server.py", line 52, in <module>
    app.run(debug=False, port=5001)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/app.py", line 620, in run
    cli.show_server_banner(self.debug, self.name)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/cli.py", line 756, in show_server_banner
    click.echo(f" * Serving Flask app '{app_import_path}'")
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/click/utils.py", line 322, in echo
    file.flush()
BrokenPipeError: [Errno 32] Broken pipe
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
BrokenPipeError: [Errno 32] Broken pipe
Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/server.py", line 52, in <module>
    app.run(debug=False, port=5001)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/app.py", line 620, in run
    cli.show_server_banner(self.debug, self.name)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/cli.py", line 756, in show_server_banner
    click.echo(f" * Serving Flask app '{app_import_path}'")
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/click/utils.py", line 322, in echo
    file.flush()
BrokenPipeError: [Errno 32] Broken pipe
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
BrokenPipeError: [Errno 32] Broken pipe
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5001
[33mPress CTRL+C to quit[0m
127.0.0.1 - - [07/Jul/2025 20:23:14] "POST /api/build_armory_index HTTP/1.1" 200 -
Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/server.py", line 52, in <module>
    app.run(debug=False, port=5001)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/app.py", line 620, in run
    cli.show_server_banner(self.debug, self.name)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/cli.py", line 756, in show_server_banner
    click.echo(f" * Serving Flask app '{app_import_path}'")
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/click/utils.py", line 322, in echo
    file.flush()
BrokenPipeError: [Errno 32] Broken pipe
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
BrokenPipeError: [Errno 32] Broken pipe
Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/server.py", line 52, in <module>
    app.run(debug=False, port=5001)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/app.py", line 620, in run
    cli.show_server_banner(self.debug, self.name)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/cli.py", line 756, in show_server_banner
    click.echo(f" * Serving Flask app '{app_import_path}'")
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/click/utils.py", line 322, in echo
    file.flush()
BrokenPipeError: [Errno 32] Broken pipe
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
BrokenPipeError: [Errno 32] Broken pipe
Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/server.py", line 52, in <module>
    app.run(debug=False, port=5001)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/app.py", line 620, in run
    cli.show_server_banner(self.debug, self.name)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/flask/cli.py", line 756, in show_server_banner
    click.echo(f" * Serving Flask app '{app_import_path}'")
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/click/utils.py", line 322, in echo
    file.flush()
BrokenPipeError: [Errno 32] Broken pipe
Exception ignored in: <_io.TextIOWrapper name='<stdout>' mode='w' encoding='utf-8'>
BrokenPipeError: [Errno 32] Broken pipe
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5001
[33mPress CTRL+C to quit[0m
127.0.0.1 - - [07/Jul/2025 20:25:31] "POST /api/build_armory_index HTTP/1.1" 200 -
[31m[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.[0m
 * Running on http://127.0.0.1:5001
[33mPress CTRL+C to quit[0m
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
127.0.0.1 - - [07/Jul/2025 20:34:42] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:34:43] "POST /api/build_armory_index HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:34:52] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:34:52] "POST /api/build_armory_index HTTP/1.1" 200 -
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
127.0.0.1 - - [07/Jul/2025 20:35:20] "POST /api/scan_project HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:36:22] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:36:22] "POST /api/build_armory_index HTTP/1.1" 200 -
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
127.0.0.1 - - [07/Jul/2025 20:38:10] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:38:10] "POST /api/build_armory_index HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:38:20] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:38:20] "POST /api/build_armory_index HTTP/1.1" 200 -
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
127.0.0.1 - - [07/Jul/2025 20:38:32] "POST /api/scan_project HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:38:50] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:42:10] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:42:11] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:42:14] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:46:11] "POST /api/enrich_repo HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:51:52] "POST /api/scan_project HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:52:53] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:53:29] "POST /api/find_solution_candidates HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:55:20] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:57:12] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 20:58:30] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 20:58:30] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 20:58:31] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 21:14:06] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:14:06] "POST /api/build_armory_index HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:14:08] "POST /api/scan_project HTTP/1.1" 200 -
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
127.0.0.1 - - [07/Jul/2025 21:14:17] "DELETE /api/delete_repo HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:15:32] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:15:34] "POST /api/set_llm_model HTTP/1.1" 200 -
Error during enrich_repo: Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/api/armory_routes.py", line 48, in enrich_repo_route
    enriched_data = enrich_repo(repo_url)
                    ^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/services/enrichment_service.py", line 135, in enrich_repo
    raise ValueError("Could not find any key technical files (README, requirements, etc.) to analyze.")
ValueError: Could not find any key technical files (README, requirements, etc.) to analyze.

127.0.0.1 - - [07/Jul/2025 21:16:09] "[35m[1mPOST /api/enrich_repo HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 21:17:39] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:17:39] "POST /api/build_armory_index HTTP/1.1" 200 -
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
Error during enrich_repo: Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpcore/_sync/connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 136, in handle_request
    raise exc
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpcore/_sync/http11.py", line 231, in _receive_event
    raise RemoteProtocolError(msg)
httpcore.RemoteProtocolError: Server disconnected without sending a response.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/api/armory_routes.py", line 48, in enrich_repo_route
    enriched_data = enrich_repo(repo_url)
                    ^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/services/enrichment_service.py", line 140, in enrich_repo
    briefing_card = program(repo_context=repo_context)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py", line 260, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/llama_index/core/program/llm_program.py", line 93, in __call__
    response = self._llm.chat(messages, **llm_kwargs)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/llama_index/core/instrumentation/dispatcher.py", line 260, in wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/llama_index/core/llms/callbacks.py", line 172, in wrapped_llm_chat
    f_return_val = f(_self, messages, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/llama_index/llms/ollama/base.py", line 131, in chat
    response = client.post(
               ^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_client.py", line 1144, in post
    return self.request(
           ^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_client.py", line 825, in request
    return self.send(request, auth=auth, follow_redirects=follow_redirects)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 249, in handle_request
    with map_httpcore_exceptions():
         ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/spiderman/anaconda3/lib/python3.12/contextlib.py", line 158, in __exit__
    self.gen.throw(value)
  File "/home/data/workspace/latihan_donny/curators_atlas/backend/curators_venv/lib/python3.12/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.RemoteProtocolError: Server disconnected without sending a response.

127.0.0.1 - - [07/Jul/2025 21:18:22] "[35m[1mPOST /api/enrich_repo HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 21:33:34] "POST /api/scan_project HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:33:45] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:33:47] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:33:49] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:34:25] "POST /api/find_solution_candidates HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:36:00] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:37:40] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:38:49] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 21:39:33] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 21:39:45] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 22:28:04] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:28:04] "POST /api/build_armory_index HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:28:07] "POST /api/scan_project HTTP/1.1" 200 -
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
127.0.0.1 - - [07/Jul/2025 22:30:28] "POST /api/scan_project HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:32:30] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:33:11] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:33:13] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:33:15] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:35:50] "POST /api/enrich_repo HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:36:12] "POST /api/scan_project HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:37:04] "POST /api/find_solution_candidates HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:38:24] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:40:06] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:41:18] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:42:06] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 22:42:06] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 22:51:14] "GET /api/get_ollama_models HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:51:14] "POST /api/build_armory_index HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:51:16] "POST /api/scan_project HTTP/1.1" 200 -
Address already in use
Port 5001 is in use by another program. Either identify and stop that program, or start the server with a different port.
127.0.0.1 - - [07/Jul/2025 22:51:26] "DELETE /api/delete_repo HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:52:11] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:52:12] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:52:14] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:54:10] "POST /api/enrich_repo HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:54:26] "POST /api/scan_project HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:54:53] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:55:27] "POST /api/find_solution_candidates HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:57:23] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:59:23] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:59:30] "POST /api/scan_project HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 22:59:34] "POST /api/set_llm_model HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 23:00:28] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 23:00:28] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 23:00:29] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 23:01:43] "POST /api/find_solution_candidates HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 23:04:03] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 23:06:06] "POST /api/run_impact_analysis HTTP/1.1" 200 -
127.0.0.1 - - [07/Jul/2025 23:06:44] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 23:06:44] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
127.0.0.1 - - [07/Jul/2025 23:06:45] "[35m[1mPOST /api/run_impact_analysis HTTP/1.1[0m" 500 -
